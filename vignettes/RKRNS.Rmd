---
title: "Spectral representations of language are reflected in brain function"
bibliography: REFERENCES.bib
output:
  pdf_document:
    toc: true
    highlight: zenburn
  ioslides_presentation:
    incremental: false
    widescreen: true
    smaller: false
  html_document:
    toc: true
    theme: readable
---
<!--
%\VignetteEngine{knitr::rmarkdown}
%\VignetteIndexEntry{Intro to RKRNS}
  revealjs_presentation:
    theme: sky
    transition: fade
    highlight: pygments
    center: true
    incremental: true
-->

# Background

## A long history of research

Comprehension from reading, like hearing (@French1947), is fast and contextual.

## Semantic Patterns in the Brain 

Does reading different sentences produce specific patterns of brain
activity over time?  Are these patterns repeatable and does their
variability relate to semantic similarity?  How do these sentence level patterns differ from word-level patterns?

## Spectral representations of language at the brain and sentence level

We show that functional activation of the left hemisphere language
network reliably decodes an individual's reading experience.  We use a
framework based on dimensionality reduction ( _eigenanatomy_ and
_eigenwords_ ) to identify brain sub-regions that differentiate
related content during an extended session of sentence reading.  A
distinguishing feature of this framework is that it makes relatively
few assumptions about the BOLD response beyond its slow (6 - 20+
second ) time-scale.  Our statistical design focuses on the event
occurrence (reading) and its quantitative representation.  The BOLD
response is encoded directly as event-matched spatiotemporal
predictors from which we derive sparse dynamic, anatomical dictionary
descriptors.  Our approach seeks to address two of the fundamental
issues at hand: (1) the need for a quantitative description of
sentences that is related to sentence comprehension from reading and
(2) a general formulation for relating the high-dimensional
spatiotemporal BOLD space to spectral sentence representations.

## Overview

Below, we first describe the proposed sentence representation.  We
then describe BOLD preprocessing.  Finally, we detail our approach to
integrating these in a predictive model. We implement the analysis in
`R` with `ANTsR` for imaging-specific tasks and SCCAN dimensionality
reduction.

```{r define variables,echo=TRUE, message=FALSE, warning=FALSE}
library(pdist)
svmresult<-0
TR<-0.5
blocklength<-450*TR
if ( exists("imat") ) nvol<-nrow(imat) else nvol<-57600
boldhours<-nvol*TR/3600
```


# The basic design

```{r setup,echo=TRUE, message=FALSE, warning=FALSE}
# setupfile<-paste("demo/",c("setup.R"),sep='')
# source(system.file( setupfile, package="RKRNS"))
opts_chunk$set(dev = 'pdf')
print("#########setup#########TODO: need to incorporate motion parameters!")
istest<-FALSE
subject<-"111157"
datadir<-paste("/Users/stnava/data/KRNS/",subject,"/",sep='')
tr<-as.numeric(0.5)
eventshift<-6
responselength<-6/tr # e.g. 15 seconds div by 0.5 tr => 30 volumes
labs<-as.numeric(1:90) # label numbers to use ... need to know which label set is at hand
labs<-as.numeric( c(13,79,81,89) ) # lang network
throwaway<-8
ncompcor<-6
compcorvarval<-0.95
filterlowfrequency<-0.05 # 0.05 if you multiply by TR
filterhighfrequency<-0.5 # 0.4 # because of expected bold response < 25secs, > 5 seconds
trendfrequency<-3
winsorval<-0.01
eigsentbasislength<-100
aalfn<-paste(datadir,"aal/",subject,"_aal-mocoref.nii.gz",sep='')
if ( file.exists(aalfn) ) aalimg<-antsImageRead( aalfn , 3 ) else stop(paste("No aalfn",aalfn))
bmaskfn<-paste(datadir,"ref/",subject,"_mask.nii.gz",sep='')
if ( file.exists(bmaskfn) ) bmask<-antsImageRead( bmaskfn , 3 )
reffn<-paste(datadir,"ref/",subject,"_mocoref.nii.gz",sep='')
reffn<-paste(datadir,"aal/",subject,"_mocoref_masked.nii.gz",sep='')
if ( file.exists(reffn) ) ref<-antsImageRead( reffn , 3 )
imagedir<-paste(datadir,"moco/",sep='')
imagepostfix<-"_moco.nii.gz"
data("aal",package="ANTsR")
blocksCSVlist<-Sys.glob(paste(datadir,"design/*csv",sep=''))   # INPUT csv list
# blocksCSVlist<-blocksCSVlist[1:20] # for testing
dfn<-paste(datadir,"assembly/assembled_design_",labs[1],"_",labs[length(labs)],"test.csv",sep='') # INPUT out csv name
afn<-paste(datadir,"assembly/assembled_aal_",labs[1],"_",labs[length(labs)],"test.mha",sep='') # INPUT out img na
filtfn<-paste(datadir,"assembly/assembled_aal_",labs[1],"_",labs[length(labs)],"testfilt.mha",sep='') # INPUT out img na
```

```{r assembledesign,echo=TRUE, message=FALSE, warning=FALSE}
assembly<-assembleDesign( blocksCSVlist, datadir, dfn, afn )
dmat<-assembly[[1]]
imat<-assembly[[2]]
usedesignrow<-assembly$usedesignrow
```

```{r assembleimageblocks,echo=TRUE, message=FALSE, warning=FALSE}
assembly2<-assembleBlocks( bmask, aalimg, labs, datadir, 
  imagepostfix,  dfn, afn, dmat, usedesignrow, imat )
dmat<-assembly2[[1]]
imat<-assembly2[[2]]
usedesignrow<-assembly2$usedesignrow
subaal<-assembly2$subaal
```


```{r organizeStudy,echo=TRUE, message=FALSE, warning=FALSE}
# 
wordinds<-39:280
sentinds<-281:ncol(dmat)
dmatw<-dmat[,wordinds]
dmats<-dmat[,sentinds]
events1<-apply( dmats     , FUN=sum, MARGIN=1 )
nschar<-as.numeric( dmat$nchar )
nevents<-events1
dmatsblock<-dmats
for ( i in 1:(responselength) ) {
    dmatsblock<-dmatsblock+ashift(dmats,c(i,0))
    nschar<-nschar+shift(nschar,1)
}
for ( i in 1:(16) ) {
    nevents<-nevents+shift(nevents,1)
}
eventss<-apply( dmatsblock, FUN=sum, MARGIN=1 )
eventtimes<-which( events1 > 0 )
eventsw<-apply( dmatw, FUN=sum, MARGIN=1 )
words<-colnames(dmatw)
data(sentences, package = "RKRNS")
data(wiki_words,package="RKRNS")
nsentences<-nrow(sentences)
fspacenames<-rep("", length(eventtimes) )
dmatsnames<-colnames(dmats)
for ( i in 1:length( eventtimes ) ) {
  fspacenames[i]<-dmatsnames[ which( dmats[eventtimes[i],  ] == 1  ) ]
}
if ( ! exists("eventdata") )
  {
  eventdata<-annotateEvents( sentences$Sentence, wiki_words$WhichWord, eventtimes, fspacenames )
  }
```

```{r eigensentences,echo=TRUE, message=FALSE, warning=FALSE}
eigsent<-eigenSentences( wiki_words, normalize=F, functiontoapply = sum )
weigsent<-whiten(eigsent)
rownames(weigsent )<-rownames(eigsent)
# eigsent<-weigsent
# map them to the event space
sentspace<-matrix(rep(NA, length(eventtimes)*ncol(eigsent) ),nrow=length(eventtimes))
# compute correlations between all sentences
sentspaceSim<-matrix(rep(NA, length(eventtimes)*nrow(eigsent) ),nrow=length(eventtimes))
for ( i in 1:length( eventtimes ) )   
  {
  sentspace[i,]<-eigsent[  which(rownames(eigsent) == fspacenames[i]  ), ]    
  sentspaceSim[i,]<-cor(eigsent[  which(rownames(eigsent) == fspacenames[i]  ), ] , t(eigsent  ))   
  }
```

```{r filtering,echo=TRUE, message=FALSE, warning=FALSE}
gbold<-ts(rowMeans(imat))
# if ( ! file.exists(filtfn) ) 
{
  imatf<-filterfMRI4KRNS( imat, tr=tr,  
    filterlowfrequency=NA,
    filterhighfrequency=NA,
    trendfrequency=4,
    trendfrequency2=20,
    removeEventOverlap=NA, removeSentLengthEffects=NA )
  antsImageWrite( as.antsImage(data.matrix(imatf)), filtfn )
} # else imatf<-as.matrix( antsImageRead( filtfn, 2 ) )
gbold2<-ts(rowMeans(imatf))

# convertTimeSeriesToSpatiotemporalFeatureSpace
# 1. for each event, extract submatrix of bold, then vectorize that matrix
featspaceOrg<-timeserieswindow2matrix( data.matrix( imatf ), subaal, eventtimes+eventshift, responselength, 0, rep(0.5,4) )
featspace<-featspaceOrg$eventmatrix
mask4d<-featspaceOrg$mask4d
rownames(featspace)<-(fspacenames)
gg<-rowMeans(featspace)
ccafeatspace<-residuals(lm(featspace~ gg ) )  #  + eventss[ eventsw > 0 ] ))
ccafeatspace<-featspace
```

```{r buildfeattemplate,echo=TRUE, message=FALSE, warning=FALSE}
score<-0
nleaveout<-2
wordoi<-'coffee'
selector<-grep(wordoi,eventdata$sentences)
for ( k in 1:20 ) {
  randsubset<-sample(c(  rep(TRUE,(length(selector)-nleaveout)),rep(FALSE,nleaveout)) )
  selectorTrain<-selector[ randsubset ]
  selectorTest<-selector[ which(!randsubset) ]
  featsoi<-ccafeatspace[  selectorTrain, ]
  featste<-ccafeatspace[  selectorTest, ]
  labels<-eventdata$sentences[ selectorTrain ]
  es<-sentspaceSim[ selectorTrain , ]
  whichcols<- colnames(dmats) %in% eventdata$sentences[selectorTrain]
  classmatrix<-data.matrix( designmat[ eventdata$eventtimes , whichcols ] )[selectorTrain,]
#  myclass<-templateBasedClassification( featsoi, (labels), featste, method="sccan" , mask=mask4d,eigsents=classmatrix )
  myclass<-templateBasedClassification( featsoi, (labels), featste, method="eanat" , mask=mask4d,eigsents=es )
  print(paste(myclass,rownames(featste)[1]))
  locscore<-0
  if ( sum((myclass == rownames(featste)[1])[1]) == 1 ) locscore<-1 else if ( sum((myclass == rownames(featste)[1])[2]) == 1 ) locscore<-0.5
  score<-score+locscore
  print(paste(k,score,score/k))
}
```


```{r validation,echo=TRUE, message=FALSE, warning=FALSE}
# in the future these should call the appropriate functions or be embedded here ...
#########################################
#  factor out some nuisance signal 
#########################################
# might pass mask4d below to get other constraints on data
ccaresults<-sccanBasedDecoder( eventdata[,], dmats, ccafeatspace[,] , sentspaceSim[,], 
   mysparse = c( -0.001, -0.9 ), nvecs=15, cthresh=250, doEanat=F, mask=mask4d, 
   smooth=0.1 , its=5,  interleave=T ,  locwordlist=c("coffee")  )

#c('blue','red','yellow','green')
# residfeats<-ccafeatspace[ssubset,] %*% data.matrix(ccaresults$ccaDictionary )
# residfeats<-residuals(lm( ccafeatspace[ssubset,] ~ residfeats ) )
# ccaresults2<-sccanBasedDecoder( eventdata[ssubset,], dmats, residfeats , sentspaceSim[ssubset,], mysparse = c( -0.15, -0.5 ), nvecs=12, cthresh=0, doEanat=F, mask=mask4d, smooth=0.0 , its=22 )
#
```

```{r validationviz,echo=TRUE, message=FALSE, warning=FALSE,fig.width=8, fig.height=6}
n<-round(sqrt(length(ccaresults$ccaobject$eig1)))+1
par(mfrow=c(n-1,n) )
vislist<-list()
for ( k in 1:length(ccaresults$ccaobject$eig1) ) {
  ccaimg<-ccaresults$ccaobject$eig1[[k]]
  kk<-spatioTemporalProjectionImage( ccaimg, sum, subaal )
  antsCopyImageInfo( subaal, kk$spaceimage )
  ImageMath(3,kk$spaceimage,"Normalize",kk$spaceimage)
  vislist<-lappend( vislist, kk$spaceimage )
  if ( k == 1 ) myestimatedhrf<-kk$timefunction
  if (mean(kk$timefunction)<0) kk$timefunction<-kk$timefunction*(-1)
  plot((kk$timefunction),type='l')
}
plotANTsImage( ref, vislist, slices='12x56x1' , thresh='0.1x1', color=rainbow( length(vislist) ) , outname="sccanBasedDecoderSliceViz.png")
# compare to
# tcca<-sparseDecom2( inmatrix=list( data.matrix(imat)[1000:10000,] , data.matrix(dmatsblock)[1000:10000,] ), nvecs=20, sparseness=c(-0.1,0.1), its=5, mycoption=2, perms=0, smooth=0.5, cthresh=c(200,0), inmask=c(subaal,NA) )
# purely spatial ... 
```

## BOLD Mechanics

The full experiment employs randomized sentence presentation in an event-related design with 
TR=`r TR` second BOLD data.  In total, we acquired `r boldhours` hours of scan time over X sessions.  Within a session, 
the subject is scanned over multiple blocks of fMRI of length `r blocklength` seconds.  We expunge the first `r throwaway` volumes.

## Task Sentences

The sentence stimuli are presented randomly and with varying delays.  Example sentences, with the total number of presentations, include: 

```{r sentences,echo=FALSE, message=FALSE, warning=FALSE}
if ( ! exists("fspacenames") ) fspacenames<-sample(rep(c(1:20),50))
tbl<-table(fspacenames)
print( tbl[ sample(dim(tbl))[1:10]] )	
```

# Eigenwords and eigensentences

## Definitions

The eigenword representation is a a quantitative, contextual semantic similarity space recently contributed by Dhillon and Ungar [link](http://www.cis.upenn.edu/~ungar/CVs/spectral_NLP.html) in @dhillon_icml12_tscca .  Eigenwords are based on the canonical correlation analysis of the adjacency matrix (FIXME word choice) of words within a large corpus (Wikipedia).  That is, eigenword representations are driven by the sentence-level context(s) within which a word appears.

## Sentence transformation

In this work, we use _products or sums of eigenwords_ as an eigensentence descriptor.

$$ e_s = \Pi_i e_i \text{   ... or ...   } e_s = \frac{1}{n} \sum_{i=1}^{i=n} e_i $$.

We have not yet carefully explored how these representations impact performance.  Currently, we simply "choose one" and proceed.

# BOLD processing and priors
```{r boldpro,echo=FALSE, message=FALSE, warning=FALSE}
afilterlowfrequency<-filterlowfrequency*TR
afilterhighfrequency<-filterhighfrequency*TR
```

## BOLD signal characteristics 

BOLD signal is information rich but noisy.  We employ three
fundamental aspects of BOLD to guide our preprocessing choices:

- Physiological noise contaminates the BOLD signal ( see compcor @Behzadi2007 )
- The BOLD response is slow ( > 5 seconds ) after a stimulus
- The response has limited temporal extent ( < 25 seconds )

## Processing decisions

Our processing therefore uses the following steps:

1. Assemble the BOLD blocks to match the overall event-related design file 
    - optionally use anatomical labeling to select subregions for assembly
    - preliminary studies use language network regions: `r aal$label_name[labs]`.
1. Learn physiological noise parameters from 1 block and apply them to the rest
1. Filter the fMRI with a Butterworth band-pass filter, extracting select mid-range frequencies
    - low frequency is `r afilterlowfrequency`
    - high frequency is `r afilterhighfrequency`
1. model sentence length effects (& motion---TODO) by linear regression

## Training & testing

Once these steps are complete we choose a training
and testing split and perform statistical modeling.

# Multivariate statistical modeling

## Formulation

Convert the BOLD to a spatiotemporal representation.  This model is of the form:

$$ Z_k = w + w_{00} x_{00} + w_{01} x_{01} + \cdots + 
 w_{0p} x_{0p} + w_{10} x_{10} + \cdots + w_{tp} x_{tp} + \epsilon$$

where $w$ represents a weighting term, $Z_k$ represents a multivariate outcome (an
_eigensentence_) at time k, $x_{lm}$ represents a BOLD measurement at time $l$
and space $m$ and $t$ represents the maximum temporal index within a
short window near event $Z_k$. 

## Matrix formulation

So, if $t=0$, then the global time
index is $k$; if $t=1$, the global time index is $k+1$, etcetera.
This formulation allows us to simultaneously model and/or select
variables in space-time.  From here, we denote the right side of the 
above equation (for all $k$) as $XW^T$ where $X$ 
has dimensions $n \times p$ and $W$ has dimensions $k \times p$. 
Similarly, $Z$ is a matrix of
_eigensentence_ representations with dimension $n \times q$. 

## Sparse canonical correlation between space-time BOLD and eigenwords

CCA maximizes $PearsonCorrelation( XW^T, ZY^T )$ where $X, W$ are as above and $Z$
and $Y$ are similarly defined.  CCA optimizes the matrices $W, Y$
operating on $X, Z$ to find a low-dimensional representation of the
data pair $( X , Z )$ in which correlation is maximal.  Following
ideas outlined in @Dhillon2014 and @Avants2014, this method can be
extended with sparsity constraints that yield rows of $W, Y$ with a
controllable number of non-zero entries.

## Predictive model

Given CCA solution matrix $W$, one may employ the low-dimensional
representation, $XW^T$, in multi-label classification.  Currently, we
employ SVM or random forests as multi-label learners for the problem:

$$L_i = f( XW^T ),$$

that is, learning a (sentence) label function from the BOLD data.

# Results

## Decoding targets

We decode all `r nrow(eigsent)` sentences.  Preliminarily, we find SVD-SVM `r svmresult` and
CCA-SVM results `r ccaresults$ccaresult`.  The SVD-SVM uses the SVD of the matrix
$X$ as predictors.

A simple example
[link](http://htmlpreview.github.io/?https://github.com/stnava/RKRNS/blob/doc/src/eigBrainLang.html)
shows the misclassification network ( see the interactive graph ).
Every sentence is represented by 2 nodes, the truth and the
prediction.  These 2 nodes have the same color.  Under perfect
classification, only these node pairs will be connected with a thick
edge.  The edge weights are proportional to the number of
classification cases.  You can zoom and click on the graph to get an
idea of what sentences are well-classified.

```{r d3,results='asis',iframe = TRUE,fig.cap='A (mis)-classification network',echo=FALSE, message=FALSE, warning=FALSE}
print("TODO")
#  ww <- classificationNetwork( nodesIn=nodedf, wclassesf[l2], pred ,outfile=NA, mycharge=-2066,zoom=T) 
```

## Temporal dynamics 

A "HRF" is estimated by CCA.
```{r hrf,fig.cap='SCCAN-Estimated HRF', message=FALSE, warning=FALSE}
mydata <- data.frame(time=c(1:length(myestimatedhrf))*TR+eventshift*TR,BOLD=myestimatedhrf)
ggplot(mydata,aes(time,BOLD))+geom_line()
```
## 

This function sometimes has an appearance that is similar to what's
expected from the literature. It is estimated by converting a row of
$W$ back to matrix representation and inspecting the variability in
weights over time.  Something similar may be done to find the
corresponding anatomical function.

## Anatomical locality  

Sparse CCA selects voxels from within the language network to maximize
predictive accuracy.  Where are these voxels?  This work is yet to be
done carefully.  Preliminary investigations (and our use of
spatiotemporally regularized SCCAN @Avants2014) show that the voxels
are locally clustered and distributed across inferior temporal lobe,
superior temporal (Heschl's?) gyrus and inferior frontal gyrus.

```{r viz,results='asis',iframe = TRUE,fig.cap='Functional-Anatomical clustering / predictors',echo=FALSE, message=FALSE, warning=FALSE}
  print("plotANTsImage( ref, vislist , slices='12x56x2' , thresh='0.25x1', color=rainbow( length(vislist) ) )")
```

# Discussion

## Problems 

There are several problems and shortcomings to this analysis.

- Eigenwords are somewhat arbitrary and may not reflect neural organization of semantic meaning
- Optimization of parameters was performed on one subject's dataset though we strived to make minimal assumptions
- Interpretability is not optimal 
- Visualization needs improvement, especially of brain activity over time
- No explicit modeling of the residual BOLD signal due to prior events

## Strengths 

Some strengths include relatively few assumptions, a flexible
implementation and open-science approach. Furthermore, for the three
sentences that contain the word "coffee", we can achieve excellent
classification levels (80-100%) in a split-half cross-validation.
Several other classification problems show performance well-above
chance with current ceilings around a factor of M times chance.

# References


